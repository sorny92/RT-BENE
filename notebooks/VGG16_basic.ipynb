{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35472663-a1e6-4228-a178-5006b2effe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43896a6-ec52-44e5-a44e-cd7fabd18f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW_PATH = \"../data/RAW/RT-BENE.zip\" \n",
    "DATA_INTER_PATH = \"../data/intermediate/\"\n",
    "DATA_PATH = f\"{DATA_INTER_PATH}/RT-BENE\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    !unzip -q $DATA_RAW_PATH $DATA_INTER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da68634c-20ed-4ada-967b-a10f34029522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb3c96d-687c-483c-a0d3-1144962d32b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blink_id</th>\n",
       "      <th>left_eye</th>\n",
       "      <th>right_eye</th>\n",
       "      <th>video</th>\n",
       "      <th>blink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_left_000001_rgb.png</td>\n",
       "      <td>0_right_000001_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0_left_000002_rgb.png</td>\n",
       "      <td>0_right_000002_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0_left_000003_rgb.png</td>\n",
       "      <td>0_right_000003_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0_left_000004_rgb.png</td>\n",
       "      <td>0_right_000004_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0_left_000005_rgb.png</td>\n",
       "      <td>0_right_000005_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107345</th>\n",
       "      <td>107345</td>\n",
       "      <td>16_left_009059_rgb.png</td>\n",
       "      <td>16_right_009059_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107346</th>\n",
       "      <td>107346</td>\n",
       "      <td>16_left_009060_rgb.png</td>\n",
       "      <td>16_right_009060_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107347</th>\n",
       "      <td>107347</td>\n",
       "      <td>16_left_009061_rgb.png</td>\n",
       "      <td>16_right_009061_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107348</th>\n",
       "      <td>107348</td>\n",
       "      <td>16_left_009062_rgb.png</td>\n",
       "      <td>16_right_009062_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107349</th>\n",
       "      <td>107349</td>\n",
       "      <td>16_left_009063_rgb.png</td>\n",
       "      <td>16_right_009063_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107350 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        blink_id                left_eye                right_eye  video  \\\n",
       "0              0   0_left_000001_rgb.png   0_right_000001_rgb.png      0   \n",
       "1              1   0_left_000002_rgb.png   0_right_000002_rgb.png      0   \n",
       "2              2   0_left_000003_rgb.png   0_right_000003_rgb.png      0   \n",
       "3              3   0_left_000004_rgb.png   0_right_000004_rgb.png      0   \n",
       "4              4   0_left_000005_rgb.png   0_right_000005_rgb.png      0   \n",
       "...          ...                     ...                      ...    ...   \n",
       "107345    107345  16_left_009059_rgb.png  16_right_009059_rgb.png     16   \n",
       "107346    107346  16_left_009060_rgb.png  16_right_009060_rgb.png     16   \n",
       "107347    107347  16_left_009061_rgb.png  16_right_009061_rgb.png     16   \n",
       "107348    107348  16_left_009062_rgb.png  16_right_009062_rgb.png     16   \n",
       "107349    107349  16_left_009063_rgb.png  16_right_009063_rgb.png     16   \n",
       "\n",
       "        blink  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "107345      0  \n",
       "107346      0  \n",
       "107347      0  \n",
       "107348      0  \n",
       "107349      0  \n",
       "\n",
       "[107350 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f\"{DATA_PATH}/blinks.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358eef2-c764-4557-86b2-104413d7c6ff",
   "metadata": {},
   "source": [
    "### How many videos do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df339bd-3747-4f21-b2a2-3821ff303e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = np.unique(data[\"video\"])\n",
    "video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f2a1e-1858-4671-a36e-8895f9c4008c",
   "metadata": {},
   "source": [
    "### Total images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc1945a-e8b2-49d4-81fe-06c44aa8364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107350"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"blink_id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248cca0-6ca5-4e58-97c0-5c80fab138ba",
   "metadata": {},
   "source": [
    "### How many images per video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86a9305-0d25-4830-8dee-50d87b091bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images in video</th>\n",
       "      <th>% blink frames</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12865</td>\n",
       "      <td>7.236689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8671</td>\n",
       "      <td>1.476185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8702</td>\n",
       "      <td>9.066881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3205</td>\n",
       "      <td>5.210608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4750</td>\n",
       "      <td>2.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5355</td>\n",
       "      <td>2.054155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1857</td>\n",
       "      <td>8.023694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6108</td>\n",
       "      <td>7.514735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4210</td>\n",
       "      <td>1.068884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16559</td>\n",
       "      <td>2.131771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12817</td>\n",
       "      <td>5.399079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>935</td>\n",
       "      <td>2.459893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9586</td>\n",
       "      <td>3.077405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5371</td>\n",
       "      <td>4.002979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1810</td>\n",
       "      <td>1.602210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4549</td>\n",
       "      <td>0.923280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Images in video  % blink frames\n",
       "video                                 \n",
       "0                12865        7.236689\n",
       "1                 8671        1.476185\n",
       "2                 8702        9.066881\n",
       "3                 3205        5.210608\n",
       "4                 4750        2.736842\n",
       "5                 5355        2.054155\n",
       "7                 1857        8.023694\n",
       "8                 6108        7.514735\n",
       "9                 4210        1.068884\n",
       "10               16559        2.131771\n",
       "11               12817        5.399079\n",
       "12                 935        2.459893\n",
       "13                9586        3.077405\n",
       "14                5371        4.002979\n",
       "15                1810        1.602210\n",
       "16                4549        0.923280"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_video = data.groupby(by=[\"video\"]).count()\n",
    "blinks_per_video = data.loc[data[\"blink\"] == 1].groupby(by=\"video\").count()\n",
    "blinks_per_video = blinks_per_video.div(data_per_video, level=\"video\") * 100\n",
    "data_per_video = pd.concat([data_per_video[\"blink_id\"], blinks_per_video[\"blink\"]], axis=1, keys=[\"Images in video\",\"% blink frames\"])\n",
    "data_per_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f11ec-797e-43d1-bd26-187feb077b43",
   "metadata": {},
   "source": [
    "Previous data shows that a rebalancing method will have to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26326774-e988-4981-9b8b-78a5c887dcb1",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e0771-9a36-4378-8798-f79866c728de",
   "metadata": {},
   "source": [
    "#### Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252c72ab-cba5-44d7-bb01-2a59eca9e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68702, 5)\n",
      "(17176, 5)\n",
      "(21470, 5)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "train_data = data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = train_data[int(train_data.shape[0]*(1-test_size)):]\n",
    "train_data = train_data[:int(train_data.shape[0]*(1-test_size)) -1]\n",
    "\n",
    "val_data = train_data[int(train_data.shape[0]*(1-val_size)):]\n",
    "train_data = train_data[:int(train_data.shape[0]*(1-val_size)) -1]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bc867-e450-423b-a8fe-56df94d8aec8",
   "metadata": {},
   "source": [
    "#### Generic generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc1e6cf-52fc-4abb-90e1-acfbfcf52836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class RTBeneDataset:\n",
    "    def __init__(self, phase: str, data: pd.DataFrame):\n",
    "        self.phase = phase\n",
    "        self.data = data\n",
    "        \n",
    "        if self.phase == \"train\":\n",
    "            #Shuffle the data\n",
    "            self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            self.data = self.data.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "    def __getitem(self, idx):\n",
    "        row = self.data.loc[idx,[\"left_eye\", \"right_eye\"]].to_list(), self.data.loc[idx,[\"blink\"]].to_list()[0]\n",
    "        return row\n",
    "        #return RTBeneDataset.load_row(DATA_PATH, row)\n",
    "    \n",
    "    def __call__(self):\n",
    "        for i in range(self.data.shape[0]):\n",
    "            yield self.__getitem(i)\n",
    "            \n",
    "            if i == (self.data.shape[0] -1):\n",
    "                # When all the dataset is readed, reshuffle again\n",
    "                self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "       \n",
    "    @staticmethod\n",
    "    def load_row(x, y):\n",
    "        print(x)\n",
    "        left_image = cv2.imread(f\"{DATA_PATH}/images/{x[0]}\")\n",
    "        right_image = cv2.imread(f\"{DATA_PATH}/images/{x[1]}\")\n",
    "        return (left_image/255, right_image/255), y\n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def tf_load_row(x, y):\n",
    "        image_l = tf.io.read_file(tf.strings.join([f\"{DATA_PATH}/images/\", x[0]]))\n",
    "        image_r = tf.io.read_file(tf.strings.join([f\"{DATA_PATH}/images/\", x[1]]))\n",
    "        image_l = tf.image.decode_png(image_l, channels=3)\n",
    "        image_r = tf.image.decode_png(image_r, channels=3)\n",
    "        return (image_l/255, image_r/255), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ecc920-9f14-4400-8484-480a0e9065c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RTB = RTBeneDataset(\"train\", train_data)\n",
    "val_RTB = RTBeneDataset(\"val\", val_data)\n",
    "test_RTB = RTBeneDataset(\"val\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd8d481-5151-4a09-bb21-e3ece76b6bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 16:27:56.739253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:56.768112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:56.768372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:56.769029: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 16:27:56.769524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:56.769779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:56.770011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:57.223482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:57.223718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:57.223917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 16:27:57.224120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7383 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:05:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(train_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).batch(batch_size).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(val_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(test_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94104655-132f-4517-b98e-5c36107623c9",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ccc5e52-356e-41cc-9434-448d84f4189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7dd581f-c27d-4bc3-9084-d45cc726e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 36, 60, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 36, 60, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " vgg_left (Functional)          (None, 1, 1, 512)    14714688    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " vgg_right (Functional)         (None, 1, 1, 512)    14714688    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1, 1, 1024)   0           ['vgg_left[0][0]',               \n",
      "                                                                  'vgg_right[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1024)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          131200      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,560,705\n",
      "Trainable params: 29,560,705\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_eye = keras.Input(shape=(36, 60, 3))\n",
    "right_eye = keras.Input(shape=(36, 60, 3))\n",
    "vgg_left = VGG16(weights=\"imagenet\", include_top=False, input_shape=(36, 60, 3))\n",
    "vgg_left._name = \"vgg_left\"\n",
    "vgg_right = VGG16(weights=\"imagenet\", include_top=False, input_shape=(36, 60, 3))\n",
    "vgg_right._name = \"vgg_right\"\n",
    "left_feat_extractor = vgg_left(left_eye)\n",
    "right_feat_extractor = vgg_right(right_eye)\n",
    "concat = keras.layers.Concatenate()([left_feat_extractor, right_feat_extractor])\n",
    "flat = keras.layers.Flatten()(concat)\n",
    "dense_1 = keras.layers.Dense(128, activation=\"relu\")(flat)\n",
    "out = keras.layers.Dense(1, activation=\"sigmoid\")(dense_1)\n",
    "\n",
    "model = keras.Model(inputs=([left_eye, right_eye]), outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5246c770-00a1-45de-a158-e019f4743882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall(), F1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccf3233-fa62-48fe-b367-6135623d9f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 16:27:59.675721: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n",
      "2022-02-26 16:27:59.916214: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536/536 [==============================] - 99s 178ms/step - loss: 0.1280 - binary_accuracy: 0.9650 - precision: 0.7126 - recall: 0.3039 - F1_score: 0.3117 - val_loss: 0.0651 - val_binary_accuracy: 0.9752 - val_precision: 0.6388 - val_recall: 0.9226 - val_F1_score: 0.7400\n",
      "Epoch 2/40\n",
      "536/536 [==============================] - 96s 177ms/step - loss: 0.0320 - binary_accuracy: 0.9890 - precision: 0.8943 - recall: 0.8417 - F1_score: 0.8483 - val_loss: 0.0266 - val_binary_accuracy: 0.9927 - val_precision: 0.9321 - val_recall: 0.8875 - val_F1_score: 0.8957\n",
      "Epoch 3/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0256 - binary_accuracy: 0.9919 - precision: 0.9279 - recall: 0.8794 - F1_score: 0.8865 - val_loss: 0.0245 - val_binary_accuracy: 0.9916 - val_precision: 0.9113 - val_recall: 0.8831 - val_F1_score: 0.8911\n",
      "Epoch 4/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.1053 - binary_accuracy: 0.9857 - precision: 0.8700 - recall: 0.7822 - F1_score: 0.7751 - val_loss: 0.0914 - val_binary_accuracy: 0.9649 - val_precision: 0.9355 - val_recall: 0.1634 - val_F1_score: 0.2414\n",
      "Epoch 5/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0523 - binary_accuracy: 0.9830 - precision: 0.8589 - recall: 0.7194 - F1_score: 0.7395 - val_loss: 0.0394 - val_binary_accuracy: 0.9858 - val_precision: 0.8714 - val_recall: 0.7722 - val_F1_score: 0.8119\n",
      "Epoch 6/40\n",
      "536/536 [==============================] - 94s 176ms/step - loss: 0.0287 - binary_accuracy: 0.9908 - precision: 0.9056 - recall: 0.8732 - F1_score: 0.8724 - val_loss: 0.0218 - val_binary_accuracy: 0.9925 - val_precision: 0.9066 - val_recall: 0.9142 - val_F1_score: 0.9048\n",
      "Epoch 7/40\n",
      "536/536 [==============================] - 94s 176ms/step - loss: 0.0234 - binary_accuracy: 0.9924 - precision: 0.9207 - recall: 0.8999 - F1_score: 0.8991 - val_loss: 0.0233 - val_binary_accuracy: 0.9928 - val_precision: 0.8917 - val_recall: 0.9394 - val_F1_score: 0.9135\n",
      "Epoch 8/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0217 - binary_accuracy: 0.9931 - precision: 0.9334 - recall: 0.9026 - F1_score: 0.9000 - val_loss: 0.0213 - val_binary_accuracy: 0.9931 - val_precision: 0.9471 - val_recall: 0.8819 - val_F1_score: 0.9032\n",
      "Epoch 9/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0187 - binary_accuracy: 0.9939 - precision: 0.9370 - recall: 0.9185 - F1_score: 0.9126 - val_loss: 0.0189 - val_binary_accuracy: 0.9939 - val_precision: 0.9328 - val_recall: 0.9183 - val_F1_score: 0.9071\n",
      "Epoch 10/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0204 - binary_accuracy: 0.9932 - precision: 0.9285 - recall: 0.9117 - F1_score: 0.9066 - val_loss: 0.0162 - val_binary_accuracy: 0.9945 - val_precision: 0.9515 - val_recall: 0.9126 - val_F1_score: 0.9117\n",
      "Epoch 11/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.0172 - binary_accuracy: 0.9940 - precision: 0.9389 - recall: 0.9191 - F1_score: 0.9198 - val_loss: 0.0183 - val_binary_accuracy: 0.9938 - val_precision: 0.9133 - val_recall: 0.9379 - val_F1_score: 0.9041\n",
      "Epoch 12/40\n",
      "536/536 [==============================] - 95s 176ms/step - loss: 0.0183 - binary_accuracy: 0.9940 - precision: 0.9390 - recall: 0.9197 - F1_score: 0.9195 - val_loss: 0.0175 - val_binary_accuracy: 0.9947 - val_precision: 0.9558 - val_recall: 0.9140 - val_F1_score: 0.9249\n",
      "Epoch 13/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.0230 - binary_accuracy: 0.9924 - precision: 0.9233 - recall: 0.8975 - F1_score: 0.8923 - val_loss: 0.0240 - val_binary_accuracy: 0.9920 - val_precision: 0.9673 - val_recall: 0.8336 - val_F1_score: 0.8825\n",
      "Epoch 14/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.0248 - binary_accuracy: 0.9921 - precision: 0.9213 - recall: 0.8905 - F1_score: 0.8918 - val_loss: 0.0203 - val_binary_accuracy: 0.9931 - val_precision: 0.9277 - val_recall: 0.9042 - val_F1_score: 0.9039\n",
      "Epoch 15/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.0178 - binary_accuracy: 0.9941 - precision: 0.9386 - recall: 0.9219 - F1_score: 0.9234 - val_loss: 0.0171 - val_binary_accuracy: 0.9943 - val_precision: 0.9310 - val_recall: 0.9323 - val_F1_score: 0.9199\n",
      "Epoch 16/40\n",
      "536/536 [==============================] - 93s 174ms/step - loss: 0.0263 - binary_accuracy: 0.9927 - precision: 0.9303 - recall: 0.8970 - F1_score: 0.8942 - val_loss: 0.0188 - val_binary_accuracy: 0.9942 - val_precision: 0.9594 - val_recall: 0.8973 - val_F1_score: 0.9032\n",
      "Epoch 17/40\n",
      "536/536 [==============================] - 94s 175ms/step - loss: 0.0164 - binary_accuracy: 0.9947 - precision: 0.9493 - recall: 0.9247 - F1_score: 0.9229 - val_loss: 0.0264 - val_binary_accuracy: 0.9941 - val_precision: 0.9033 - val_recall: 0.9592 - val_F1_score: 0.9180\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_id = f'{model.name}-{datetime.now().strftime(\"%m-%H%M%S\")}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"models/{run_id}/\" + \"{epoch:02d}-{val_F1_score:.2f}\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_F1_score',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{run_id}\", update_freq=100,)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_F1_score\",\n",
    "    min_delta=0.01,\n",
    "    mode=\"max\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=40, \n",
    "          steps_per_epoch=int(train_data.shape[0]/batch_size), \n",
    "          validation_steps=int(val_data.shape[0]/batch_size),\n",
    "          callbacks=[model_checkpoint_callback, tensorboard_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a994f3-5e62-49fe-b010-cd942a7bbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 17s 101ms/step - loss: 0.0174 - binary_accuracy: 0.9941 - precision: 0.9550 - recall: 0.9048 - F1_score: 0.9254\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0a960-fdd7-459c-84cd-d6165ad917e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
