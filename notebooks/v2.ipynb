{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35472663-a1e6-4228-a178-5006b2effe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43896a6-ec52-44e5-a44e-cd7fabd18f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW_PATH = \"../data/RAW/RT-BENE.zip\" \n",
    "DATA_INTER_PATH = \"/opt/data/Ubuntu/projects/RT-BENE/data/intermediate\"\n",
    "DATA_PATH = f\"{DATA_INTER_PATH}/RT-BENE\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    !unzip -q $DATA_RAW_PATH -d $DATA_INTER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da68634c-20ed-4ada-967b-a10f34029522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb3c96d-687c-483c-a0d3-1144962d32b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blink_id</th>\n",
       "      <th>left_eye</th>\n",
       "      <th>right_eye</th>\n",
       "      <th>video</th>\n",
       "      <th>blink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_left_000001_rgb.png</td>\n",
       "      <td>0_right_000001_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0_left_000002_rgb.png</td>\n",
       "      <td>0_right_000002_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0_left_000003_rgb.png</td>\n",
       "      <td>0_right_000003_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0_left_000004_rgb.png</td>\n",
       "      <td>0_right_000004_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0_left_000005_rgb.png</td>\n",
       "      <td>0_right_000005_rgb.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107345</th>\n",
       "      <td>107345</td>\n",
       "      <td>16_left_009059_rgb.png</td>\n",
       "      <td>16_right_009059_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107346</th>\n",
       "      <td>107346</td>\n",
       "      <td>16_left_009060_rgb.png</td>\n",
       "      <td>16_right_009060_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107347</th>\n",
       "      <td>107347</td>\n",
       "      <td>16_left_009061_rgb.png</td>\n",
       "      <td>16_right_009061_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107348</th>\n",
       "      <td>107348</td>\n",
       "      <td>16_left_009062_rgb.png</td>\n",
       "      <td>16_right_009062_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107349</th>\n",
       "      <td>107349</td>\n",
       "      <td>16_left_009063_rgb.png</td>\n",
       "      <td>16_right_009063_rgb.png</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107350 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        blink_id                left_eye                right_eye  video  \\\n",
       "0              0   0_left_000001_rgb.png   0_right_000001_rgb.png      0   \n",
       "1              1   0_left_000002_rgb.png   0_right_000002_rgb.png      0   \n",
       "2              2   0_left_000003_rgb.png   0_right_000003_rgb.png      0   \n",
       "3              3   0_left_000004_rgb.png   0_right_000004_rgb.png      0   \n",
       "4              4   0_left_000005_rgb.png   0_right_000005_rgb.png      0   \n",
       "...          ...                     ...                      ...    ...   \n",
       "107345    107345  16_left_009059_rgb.png  16_right_009059_rgb.png     16   \n",
       "107346    107346  16_left_009060_rgb.png  16_right_009060_rgb.png     16   \n",
       "107347    107347  16_left_009061_rgb.png  16_right_009061_rgb.png     16   \n",
       "107348    107348  16_left_009062_rgb.png  16_right_009062_rgb.png     16   \n",
       "107349    107349  16_left_009063_rgb.png  16_right_009063_rgb.png     16   \n",
       "\n",
       "        blink  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "107345      0  \n",
       "107346      0  \n",
       "107347      0  \n",
       "107348      0  \n",
       "107349      0  \n",
       "\n",
       "[107350 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f\"{DATA_PATH}/blinks.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358eef2-c764-4557-86b2-104413d7c6ff",
   "metadata": {},
   "source": [
    "### How many videos do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df339bd-3747-4f21-b2a2-3821ff303e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = np.unique(data[\"video\"])\n",
    "video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f2a1e-1858-4671-a36e-8895f9c4008c",
   "metadata": {},
   "source": [
    "### Total images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc1945a-e8b2-49d4-81fe-06c44aa8364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107350"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"blink_id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248cca0-6ca5-4e58-97c0-5c80fab138ba",
   "metadata": {},
   "source": [
    "### How many images per video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86a9305-0d25-4830-8dee-50d87b091bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images in video</th>\n",
       "      <th>% blink frames</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12865</td>\n",
       "      <td>7.236689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8671</td>\n",
       "      <td>1.476185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8702</td>\n",
       "      <td>9.066881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3205</td>\n",
       "      <td>5.210608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4750</td>\n",
       "      <td>2.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5355</td>\n",
       "      <td>2.054155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1857</td>\n",
       "      <td>8.023694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6108</td>\n",
       "      <td>7.514735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4210</td>\n",
       "      <td>1.068884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16559</td>\n",
       "      <td>2.131771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12817</td>\n",
       "      <td>5.399079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>935</td>\n",
       "      <td>2.459893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9586</td>\n",
       "      <td>3.077405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5371</td>\n",
       "      <td>4.002979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1810</td>\n",
       "      <td>1.602210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4549</td>\n",
       "      <td>0.923280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Images in video  % blink frames\n",
       "video                                 \n",
       "0                12865        7.236689\n",
       "1                 8671        1.476185\n",
       "2                 8702        9.066881\n",
       "3                 3205        5.210608\n",
       "4                 4750        2.736842\n",
       "5                 5355        2.054155\n",
       "7                 1857        8.023694\n",
       "8                 6108        7.514735\n",
       "9                 4210        1.068884\n",
       "10               16559        2.131771\n",
       "11               12817        5.399079\n",
       "12                 935        2.459893\n",
       "13                9586        3.077405\n",
       "14                5371        4.002979\n",
       "15                1810        1.602210\n",
       "16                4549        0.923280"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_video = data.groupby(by=[\"video\"]).count()\n",
    "blinks_per_video = data.loc[data[\"blink\"] == 1].groupby(by=\"video\").count()\n",
    "blinks_per_video = blinks_per_video.div(data_per_video, level=\"video\") * 100\n",
    "data_per_video = pd.concat([data_per_video[\"blink_id\"], blinks_per_video[\"blink\"]], axis=1, keys=[\"Images in video\",\"% blink frames\"])\n",
    "data_per_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f11ec-797e-43d1-bd26-187feb077b43",
   "metadata": {},
   "source": [
    "Previous data shows that a rebalancing method will have to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26326774-e988-4981-9b8b-78a5c887dcb1",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "To have a proper test partition we are going to separate the dataset in two sets of videos. One will be used for training and the other one for testing.\n",
    "We will select the videos ids 13, 14, 16 as the testing videos because it's images represent around 20% of the total dataset and the percentage of blinks is similar to the rest of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e0771-9a36-4378-8798-f79866c728de",
   "metadata": {},
   "source": [
    "#### Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252c72ab-cba5-44d7-bb01-2a59eca9e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 15]\n",
      "validation: [8]\n",
      "testing: [13, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "testing_ids = [13,14,16]\n",
    "validation_ids = [8]\n",
    "training_ids = np.delete(video_ids, np.array(testing_ids)-1)\n",
    "training_ids = np.delete(training_ids, np.array(validation_ids)-1)\n",
    "training_ids = training_ids.tolist()\n",
    "print(f\"train: {training_ids}\\nvalidation: {validation_ids}\\ntesting: {testing_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8601f926-4522-4fe3-bb07-d2819cfebf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 81736\n",
      "val_data: 6108\n",
      "test_data: 19506\n"
     ]
    }
   ],
   "source": [
    "train_data = data.loc[data[\"video\"].isin(training_ids)]\n",
    "print(f\"train_data: {train_data.shape[0]}\")\n",
    "val_data = data.loc[data[\"video\"].isin(validation_ids)]\n",
    "print(f\"val_data: {val_data.shape[0]}\")\n",
    "test_data = data.loc[data[\"video\"].isin(testing_ids)]\n",
    "print(f\"test_data: {test_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bc867-e450-423b-a8fe-56df94d8aec8",
   "metadata": {},
   "source": [
    "#### Generic generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc1e6cf-52fc-4abb-90e1-acfbfcf52836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class RTBeneDataset:\n",
    "    def __init__(self, phase: str, data: pd.DataFrame, mean: float, std: float, transforms = None):\n",
    "        self.phase = phase\n",
    "        self.data = data\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        if self.phase == \"train\":\n",
    "            #Shuffle the data\n",
    "            self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            self.data = self.data.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "    def __getitem(self, idx):\n",
    "        row = self.data.loc[idx,[\"left_eye\", \"right_eye\"]].to_list(), self.data.loc[idx,[\"blink\"]].to_list()[0]\n",
    "        return row\n",
    "    \n",
    "    def __call__(self):\n",
    "        for i in range(self.data.shape[0]):\n",
    "            yield self.__getitem(i)\n",
    "            \n",
    "            if i == (self.data.shape[0] -1):\n",
    "                # When all the dataset is readed, reshuffle again\n",
    "                self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "       \n",
    "    @staticmethod\n",
    "    def load_row(x, y):\n",
    "        print(x)\n",
    "        left_image = cv2.imread(f\"{DATA_PATH}/images/{x[0]}\")\n",
    "        right_image = cv2.imread(f\"{DATA_PATH}/images/{x[1]}\")\n",
    "        return (left_image/255, right_image/255), y\n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def tf_load_row(x, y):\n",
    "        image_l = tf.io.read_file(tf.strings.join([f\"{DATA_PATH}/images/\", x[0]]))\n",
    "        image_r = tf.io.read_file(tf.strings.join([f\"{DATA_PATH}/images/\", x[1]]))\n",
    "        image_l = tf.image.decode_png(image_l, channels=3)\n",
    "        image_r = tf.image.decode_png(image_r, channels=3)\n",
    "        return (image_l/255, image_r/255), y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfba562-db17-48c7-bc33-5689a30f8567",
   "metadata": {},
   "source": [
    "## Resampling to get better distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71ecc920-9f14-4400-8484-480a0e9065c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blink_RTB = RTBeneDataset(\"train\", train_data.loc[train_data[\"blink\"] == 1], 127.5, 1)\n",
    "train_no_blink_RTB = RTBeneDataset(\"train\", train_data.loc[train_data[\"blink\"] == 0], 127.5, 1)\n",
    "val_RTB = RTBeneDataset(\"val\", val_data, 127.5, 1)\n",
    "test_RTB = RTBeneDataset(\"val\", test_data, 127.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccd8d481-5151-4a09-bb21-e3ece76b6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "blink_dataset = tf.data.Dataset.from_generator(train_blink_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).repeat()\n",
    "\n",
    "no_blink_dataset = tf.data.Dataset.from_generator(train_no_blink_RTB, \n",
    "                                                  output_types=(tf.string, tf.int32), \n",
    "                                                  output_shapes=((2),())).repeat()\n",
    "\n",
    "train_dataset  = tf.data.Dataset.sample_from_datasets(\n",
    "    [blink_dataset, no_blink_dataset], [0.5, 0.5]).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).repeat()\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(val_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(test_RTB, \n",
    "                                               output_types=(tf.string, tf.int32), \n",
    "                                               output_shapes=((2),())).map(RTBeneDataset.tf_load_row, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94104655-132f-4517-b98e-5c36107623c9",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ccc5e52-356e-41cc-9434-448d84f4189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, initializers, layers\n",
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c7dd581f-c27d-4bc3-9084-d45cc726e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 1s 0us/step\n",
      "29097984/29084464 [==============================] - 1s 0us/step\n",
      "Model: \"resnet_all_data_rebalanced_dropout\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_81 (InputLayer)          [(None, 36, 60, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " input_82 (InputLayer)          [(None, 36, 60, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " feat_left (Functional)         (None, 1, 1, 1024)   7037504     ['input_81[0][0]']               \n",
      "                                                                                                  \n",
      " feat_right (Functional)        (None, 1, 1, 1024)   7037504     ['input_82[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 1, 1, 2048)   0           ['feat_left[0][0]',              \n",
      "                                                                  'feat_right[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_19 (Flatten)           (None, 2048)         0           ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 256)          524544      ['flatten_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256)          0           ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 128)          32896       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 128)          0           ['dense_49[0][0]']               \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 1)            129         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,632,577\n",
      "Trainable params: 557,569\n",
      "Non-trainable params: 14,075,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_eye = keras.Input(shape=(36, 60, 3))\n",
    "right_eye = keras.Input(shape=(36, 60, 3))\n",
    "feature_extractor_left = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(36, 60, 3))\n",
    "feature_extractor_left._name = \"feat_left\"\n",
    "for layer in feature_extractor_left.layers:\n",
    "    layer.trainable=False\n",
    "feature_extractor_right = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(36, 60, 3))\n",
    "feature_extractor_right._name = \"feat_right\"\n",
    "for layer in feature_extractor_right.layers:\n",
    "    layer.trainable=False\n",
    "left_feat_extractor = feature_extractor_left(left_eye)\n",
    "right_feat_extractor = feature_extractor_right(right_eye)\n",
    "concat = layers.Concatenate()([left_feat_extractor, right_feat_extractor])\n",
    "flat = layers.Flatten()(concat)\n",
    "dense_1 = layers.Dense(256, activation=\"relu\", \n",
    "                       kernel_regularizer=regularizers.l1(1e-5),\n",
    "                       kernel_initializer=initializers.GlorotNormal)(flat)\n",
    "do_1 = layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = layers.Dense(128, activation=\"relu\", \n",
    "                       kernel_regularizer=regularizers.l1(1e-5),\n",
    "                       kernel_initializer=initializers.GlorotNormal)(do_1)\n",
    "do_2 = layers.Dropout(0.2)(dense_2)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(do_2)\n",
    "\n",
    "model = keras.Model(inputs=([left_eye, right_eye]), outputs=out, name=\"resnet_all_data_rebalanced_dropout\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5246c770-00a1-45de-a158-e019f4743882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall(), F1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2ccf3233-fa62-48fe-b367-6135623d9f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "638/638 [==============================] - 82s 116ms/step - loss: 0.1680 - binary_accuracy: 0.9641 - precision_18: 0.9622 - recall_18: 0.9660 - F1_score: 0.9635 - val_loss: 0.3117 - val_binary_accuracy: 0.9106 - val_precision_18: 0.4177 - val_recall_18: 0.5033 - val_F1_score: 0.4447 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "638/638 [==============================] - 73s 115ms/step - loss: 0.0979 - binary_accuracy: 0.9852 - precision_18: 0.9827 - recall_18: 0.9877 - F1_score: 0.9851 - val_loss: 0.4545 - val_binary_accuracy: 0.9217 - val_precision_18: 0.4437 - val_recall_18: 0.1567 - val_F1_score: 0.2148 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "638/638 [==============================] - 78s 122ms/step - loss: 0.0838 - binary_accuracy: 0.9882 - precision_18: 0.9858 - recall_18: 0.9907 - F1_score: 0.9881 - val_loss: 0.4521 - val_binary_accuracy: 0.9149 - val_precision_18: 0.3628 - val_recall_18: 0.1822 - val_F1_score: 0.2293 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "638/638 [==============================] - 74s 116ms/step - loss: 0.0732 - binary_accuracy: 0.9904 - precision_18: 0.9879 - recall_18: 0.9928 - F1_score: 0.9903 - val_loss: 0.8045 - val_binary_accuracy: 0.8265 - val_precision_18: 0.1408 - val_recall_18: 0.2566 - val_F1_score: 0.1761 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "638/638 [==============================] - 77s 121ms/step - loss: 0.0690 - binary_accuracy: 0.9909 - precision_18: 0.9886 - recall_18: 0.9934 - F1_score: 0.9909 - val_loss: 0.4513 - val_binary_accuracy: 0.9006 - val_precision_18: 0.3363 - val_recall_18: 0.3319 - val_F1_score: 0.3249 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "638/638 [==============================] - 73s 115ms/step - loss: 0.0625 - binary_accuracy: 0.9920 - precision_18: 0.9898 - recall_18: 0.9943 - F1_score: 0.9920 - val_loss: 0.4105 - val_binary_accuracy: 0.8993 - val_precision_18: 0.3046 - val_recall_18: 0.2655 - val_F1_score: 0.2732 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "638/638 [==============================] - 74s 116ms/step - loss: 0.0467 - binary_accuracy: 0.9971 - precision_18: 0.9954 - recall_18: 0.9988 - F1_score: 0.9970 - val_loss: 0.4995 - val_binary_accuracy: 0.9116 - val_precision_18: 0.3361 - val_recall_18: 0.1814 - val_F1_score: 0.2240 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "638/638 [==============================] - 75s 118ms/step - loss: 0.0429 - binary_accuracy: 0.9975 - precision_18: 0.9961 - recall_18: 0.9989 - F1_score: 0.9975 - val_loss: 0.5462 - val_binary_accuracy: 0.9036 - val_precision_18: 0.3095 - val_recall_18: 0.2301 - val_F1_score: 0.2519 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "638/638 [==============================] - 78s 122ms/step - loss: 0.0402 - binary_accuracy: 0.9978 - precision_18: 0.9966 - recall_18: 0.9991 - F1_score: 0.9978 - val_loss: 0.5055 - val_binary_accuracy: 0.8991 - val_precision_18: 0.2911 - val_recall_18: 0.2389 - val_F1_score: 0.2483 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "638/638 [==============================] - 75s 117ms/step - loss: 0.0377 - binary_accuracy: 0.9981 - precision_18: 0.9971 - recall_18: 0.9991 - F1_score: 0.9981 - val_loss: 0.5684 - val_binary_accuracy: 0.9028 - val_precision_18: 0.2730 - val_recall_18: 0.1770 - val_F1_score: 0.2018 - lr: 2.0000e-04\n",
      "Epoch 11/20\n",
      "638/638 [==============================] - 78s 122ms/step - loss: 0.0366 - binary_accuracy: 0.9979 - precision_18: 0.9969 - recall_18: 0.9989 - F1_score: 0.9979 - val_loss: 0.6115 - val_binary_accuracy: 0.9091 - val_precision_18: 0.2889 - val_recall_18: 0.1438 - val_F1_score: 0.1825 - lr: 2.0000e-04\n",
      "Epoch 12/20\n",
      "638/638 [==============================] - 79s 124ms/step - loss: 0.0330 - binary_accuracy: 0.9987 - precision_18: 0.9976 - recall_18: 0.9998 - F1_score: 0.9987 - val_loss: 0.6154 - val_binary_accuracy: 0.9069 - val_precision_18: 0.2692 - val_recall_18: 0.1394 - val_F1_score: 0.1727 - lr: 4.0000e-05\n",
      "Epoch 13/20\n",
      "638/638 [==============================] - 77s 121ms/step - loss: 0.0318 - binary_accuracy: 0.9989 - precision_18: 0.9981 - recall_18: 0.9998 - F1_score: 0.9990 - val_loss: 0.6130 - val_binary_accuracy: 0.9054 - val_precision_18: 0.2612 - val_recall_18: 0.1416 - val_F1_score: 0.1730 - lr: 4.0000e-05\n",
      "Epoch 14/20\n",
      "638/638 [==============================] - 75s 117ms/step - loss: 0.0319 - binary_accuracy: 0.9987 - precision_18: 0.9978 - recall_18: 0.9996 - F1_score: 0.9987 - val_loss: 0.6070 - val_binary_accuracy: 0.9033 - val_precision_18: 0.2743 - val_recall_18: 0.1748 - val_F1_score: 0.2020 - lr: 4.0000e-05\n",
      "Epoch 15/20\n",
      "638/638 [==============================] - 75s 117ms/step - loss: 0.0308 - binary_accuracy: 0.9989 - precision_18: 0.9980 - recall_18: 0.9999 - F1_score: 0.9989 - val_loss: 0.6318 - val_binary_accuracy: 0.9056 - val_precision_18: 0.2738 - val_recall_18: 0.1604 - val_F1_score: 0.1961 - lr: 4.0000e-05\n",
      "Epoch 16/20\n",
      "638/638 [==============================] - 77s 120ms/step - loss: 0.0299 - binary_accuracy: 0.9990 - precision_18: 0.9982 - recall_18: 0.9998 - F1_score: 0.9990 - val_loss: 0.6402 - val_binary_accuracy: 0.9049 - val_precision_18: 0.2700 - val_recall_18: 0.1574 - val_F1_score: 0.1885 - lr: 4.0000e-05\n",
      "Epoch 17/20\n",
      "638/638 [==============================] - 76s 119ms/step - loss: 0.0297 - binary_accuracy: 0.9990 - precision_18: 0.9981 - recall_18: 1.0000 - F1_score: 0.9990 - val_loss: 0.6701 - val_binary_accuracy: 0.9066 - val_precision_18: 0.2753 - val_recall_18: 0.1508 - val_F1_score: 0.1858 - lr: 8.0000e-06\n",
      "Epoch 18/20\n",
      "638/638 [==============================] - 75s 118ms/step - loss: 0.0290 - binary_accuracy: 0.9992 - precision_18: 0.9986 - recall_18: 0.9998 - F1_score: 0.9992 - val_loss: 0.6647 - val_binary_accuracy: 0.9059 - val_precision_18: 0.2763 - val_recall_18: 0.1574 - val_F1_score: 0.1928 - lr: 8.0000e-06\n",
      "Epoch 19/20\n",
      "638/638 [==============================] - 76s 120ms/step - loss: 0.0294 - binary_accuracy: 0.9991 - precision_18: 0.9984 - recall_18: 0.9999 - F1_score: 0.9991 - val_loss: 0.6687 - val_binary_accuracy: 0.9051 - val_precision_18: 0.2659 - val_recall_18: 0.1479 - val_F1_score: 0.1817 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "638/638 [==============================] - 77s 120ms/step - loss: 0.0293 - binary_accuracy: 0.9991 - precision_18: 0.9984 - recall_18: 0.9998 - F1_score: 0.9991 - val_loss: 0.6601 - val_binary_accuracy: 0.9039 - val_precision_18: 0.2614 - val_recall_18: 0.1527 - val_F1_score: 0.1832 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_id = f'{model.name}-{datetime.now().strftime(\"%m-%H%M%S\")}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"models/{run_id}/\" + \"{epoch:02d}-{val_F1_score:.2f}\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_F1_score',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{run_id}\", update_freq=100,)\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=20, \n",
    "          steps_per_epoch=int(train_data.shape[0]/batch_size), \n",
    "          validation_steps=int(val_data.shape[0]/batch_size),\n",
    "          callbacks=[model_checkpoint_callback, tensorboard_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f8a2f845-b418-48e1-bfcc-2f96dccf675f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f45483d6a90>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"models/vgg_data_rebalanced-02-131444/11-0.82\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7a994f3-5e62-49fe-b010-cd942a7bbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 15s 100ms/step - loss: 0.3591 - binary_accuracy: 0.8626 - precision_10: 0.1613 - recall_10: 0.9185 - F1_score: 0.2605\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0a960-fdd7-459c-84cd-d6165ad917e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
